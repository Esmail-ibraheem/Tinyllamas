# X-LlamaðŸ¦™: Extensible Language Model inspired by the original Llama model.
 
<p align="center">
  <img src="https://github.com/Esmail-ibraheem/FeedbackTransformer/blob/main/llama2.jpg" alt="Your Image Description" width="400" height=400">
</p>


X-Llama is an advanced language model framework, inspired by the original Llama model but enhanced with additional features such as Grouped Query Attention (GQA), Multi-Head Attention (MHA), and more. This project aims to provide a flexible and extensible platform for experimenting with various attention mechanisms and building state-of-the-art natural language processing models.

## Features:
- **`Rotary Embeddings`:**
   - Llama Rotary Embeddings.
   - Llama Linear Scaling Rotary Embeddings.
   - Llama Dynamic NTK Scaling Rotary Embeddings.
<p align="center">
  <img src="https://github.com/Esmail-ibraheem/X-Llama/blob/main/images/RoPE.png" alt="Your Image Description">
</p>

- **`LlamaChat`.**
- **`Multi-Head Attention(MHA)`:**
- **`Grouped Query Attention(GQA)`:**
- **`Multi-Query Attention(MQA)`:**
- **`Flash-Attention`:**

---

## BibTex
```BibTex
@article{Gumaan2024-X-Llama,
  title   = "X-Llama",
  author  = "Gumaan, Esmail",
  code = "https://github.com/Esmail-ibraheem/X-Llama/tree/main",
  year    = "2024",
  month   = "Agu",
  url     = ""
}

```
---
## Notes:
